{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDR & PESQ Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to open the input \"/home/ytang363/7100_voiceConversion/eval_audio/002_p225xp225.wav\" (No such file or directory).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ytang363/7100_voiceConversion/metric-SDR_PESQ.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.207.59.29/home/ytang363/7100_voiceConversion/metric-SDR_PESQ.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m ori_waveform, ori_sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(original_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.207.59.29/home/ytang363/7100_voiceConversion/metric-SDR_PESQ.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m clean_waveform, clean_sample_rate \u001b[39m=\u001b[39m ori_waveform, ori_sample_rate\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B130.207.59.29/home/ytang363/7100_voiceConversion/metric-SDR_PESQ.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m hifi_waveform, hifi_sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(autoVC_vocoder) \u001b[39m# change here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.207.59.29/home/ytang363/7100_voiceConversion/metric-SDR_PESQ.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Convert the audio waveforms to PyTorch tensors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.207.59.29/home/ytang363/7100_voiceConversion/metric-SDR_PESQ.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m mix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ori_waveform\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[39mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m backend \u001b[39m=\u001b[39m dispatcher(uri, \u001b[39mformat\u001b[39m, backend)\n\u001b[0;32m--> 204\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39;49m, buffer_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchaudio/_backend/ffmpeg.py:336\u001b[0m, in \u001b[0;36mFFmpegBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[39mreturn\u001b[39;00m load_audio_fileobj(\n\u001b[1;32m    327\u001b[0m         uri,\n\u001b[1;32m    328\u001b[0m         frame_offset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m         buffer_size,\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m load_audio(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mnormpath(uri), frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchaudio/_backend/ffmpeg.py:100\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_audio\u001b[39m(\n\u001b[1;32m     92\u001b[0m     src: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     93\u001b[0m     frame_offset: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mformat\u001b[39m: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     98\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m     99\u001b[0m     \u001b[39mfilter\u001b[39m \u001b[39m=\u001b[39m _get_load_filter(frame_offset, num_frames, convert)\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mtorchaudio\u001b[39m.\u001b[39;49mcompat_load(src, \u001b[39mformat\u001b[39;49m, \u001b[39mfilter\u001b[39;49m, channels_first)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"/home/ytang363/7100_voiceConversion/eval_audio/002_p225xp225.wav\" (No such file or directory)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import pprint\n",
    "import numpy as np\n",
    "from asteroid.metrics import get_metrics\n",
    "\n",
    "# rootDir = '/home/ytang363/7100_voiceConversion/VCTK-Corpus-0.92/wav-16k'\n",
    "# dirName, subdirList, _ = next(os.walk(rootDir))\n",
    "# process_uttr = '001'\n",
    "# process_speakers = ['p225', 'p226', 'p227', 'p228', 'p229', 'p230', 'p231', 'p232', 'p233', 'p234', 'p237']\n",
    "\n",
    "# for speaker in sorted(subdirList):\n",
    "#     if speaker not in process_speakers:\n",
    "#         continue\n",
    "\n",
    "#     print('Processing speaker: %s' % speaker)\n",
    "#     _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))\n",
    "\n",
    "#     indices = [i for i, element in enumerate(fileList) if process_uttr in element]\n",
    "#     print(fileList[indices[0]]) \n",
    "\n",
    "#     wav_path = os.path.join(dirName, speaker, fileList[indices[0]])\n",
    "#     print(wav_path)\n",
    "\n",
    "original_path = '/home/ytang363/7100_voiceConversion/VCTK-Corpus-0.92/wav-16k/p225/p225_002_mic1.wav'\n",
    "hifiGAN_path = '/home/ytang363/7100_voiceConversion/generated_files_from_mel/p225xp225_generated_e2e.wav'\n",
    "autoVC_vocoder = '/home/ytang363/7100_voiceConversion/eval_audio-WaveNet/002_p225xp225.wav'\n",
    "\n",
    "ori_waveform, ori_sample_rate = torchaudio.load(original_path)\n",
    "clean_waveform, clean_sample_rate = ori_waveform, ori_sample_rate\n",
    "hifi_waveform, hifi_sample_rate = torchaudio.load(autoVC_vocoder) # change here\n",
    "\n",
    "# Convert the audio waveforms to PyTorch tensors\n",
    "mix = torch.tensor(ori_waveform.numpy())\n",
    "clean = torch.tensor(clean_waveform.numpy())\n",
    "est = torch.tensor(hifi_waveform.numpy())\n",
    "\n",
    "mix = ori_waveform.numpy()\n",
    "clean = clean_waveform.numpy()\n",
    "est = hifi_waveform.numpy()\n",
    "\n",
    "# Ensure the audio signals have the same length\n",
    "min_length = min(mix.shape[1], clean.shape[1], est.shape[1])\n",
    "mix = mix[:,:min_length]\n",
    "clean = clean[:,:min_length]\n",
    "est = est[:,:min_length]\n",
    "\n",
    "metrics_dict = get_metrics(mix, clean, est, sample_rate=16000,\n",
    "                            metrics_list=['sdr', 'pesq'])\n",
    "pprint.pprint(metrics_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
